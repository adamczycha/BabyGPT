{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T10:24:39.234863Z",
     "iopub.status.busy": "2024-08-28T10:24:39.233849Z",
     "iopub.status.idle": "2024-08-28T10:24:45.361943Z",
     "shell.execute_reply": "2024-08-28T10:24:45.361022Z",
     "shell.execute_reply.started": "2024-08-28T10:24:39.234820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from time import time\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T10:24:45.364162Z",
     "iopub.status.busy": "2024-08-28T10:24:45.363715Z",
     "iopub.status.idle": "2024-08-28T10:24:45.428120Z",
     "shell.execute_reply": "2024-08-28T10:24:45.426938Z",
     "shell.execute_reply.started": "2024-08-28T10:24:45.364133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T10:24:45.430424Z",
     "iopub.status.busy": "2024-08-28T10:24:45.429714Z",
     "iopub.status.idle": "2024-08-28T10:25:00.971314Z",
     "shell.execute_reply": "2024-08-28T10:25:00.970163Z",
     "shell.execute_reply.started": "2024-08-28T10:24:45.430368Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T10:25:00.974471Z",
     "iopub.status.busy": "2024-08-28T10:25:00.974106Z",
     "iopub.status.idle": "2024-08-28T10:25:00.983666Z",
     "shell.execute_reply": "2024-08-28T10:25:00.982889Z",
     "shell.execute_reply.started": "2024-08-28T10:25:00.974440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "warmup_steps = 10\n",
    "max_steps = 50\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "\n",
    "\n",
    "def get_lr(it):\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it + 1) / warmup_steps\n",
    "\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T10:25:00.985530Z",
     "iopub.status.busy": "2024-08-28T10:25:00.985193Z",
     "iopub.status.idle": "2024-08-28T10:25:01.028549Z",
     "shell.execute_reply": "2024-08-28T10:25:01.027498Z",
     "shell.execute_reply.started": "2024-08-28T10:25:00.985504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_head: int = 12\n",
    "    n_layer: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "\n",
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        #         att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        #         att = att.masked_fill(self.bias[:,:,:T,:T] == 0, (-np.inf))\n",
    "        #         att = F.softmax(att, dim = -1)\n",
    "        #         y = att @ v # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CasualSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f=nn.LayerNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        assert (\n",
    "            T <= self.config.block_size\n",
    "        ), f\"Model cannot operate {T} as a block size maximum is {self.config.block_size}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        idx = pos_emb + tok_emb\n",
    "        for block in self.transformer.h:\n",
    "            idx = block(idx)\n",
    "        idx = self.transformer.ln_f(idx)\n",
    "        logits = self.lm_head(idx)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "\n",
    "        print(f\"loading weights from pretrained gpt: {model_type}\")\n",
    "        config_args = {\n",
    "            \"gpt2\": dict(n_layer=12, n_head=12, n_embd=768),\n",
    "            \"gp2-medium\": dict(n_layer=24, n_head=16, n_embd=1024),\n",
    "            \"gpt2-large\": dict(n_layer=36, n_head=20, n_embd=1280),\n",
    "            \"gpt2-xl\": dict(n_layer=48, n_head=25, n_embd=1600),\n",
    "        }[model_type]\n",
    "        config_args[\"vocab_size\"] = 50257\n",
    "        config_args[\"block_size\"] = 1024\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [\n",
    "            k for k in sd_keys if not k.endswith(\".attn.bias\")\n",
    "        ]  # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [\n",
    "            k for k in sd_keys_hf if not k.endswith(\".attn.masked_bias\")\n",
    "        ]  # ignore these, just a buffer\n",
    "        sd_keys_hf = [\n",
    "            k for k in sd_keys_hf if not k.endswith(\".attn.bias\")\n",
    "        ]  # same, just the mask (buffer)\n",
    "        transposed = [\n",
    "            \"attn.c_attn.weight\",\n",
    "            \"attn.c_proj.weight\",\n",
    "            \"mlp.c_fc.weight\",\n",
    "            \"mlp.c_proj.weight\",\n",
    "        ]\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(\n",
    "            sd_keys\n",
    "        ), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizer(self, weight_decay, learning_rate=6e-4, device=device):\n",
    "        param_dict = {name: param for name, param in self.named_parameters()}\n",
    "        param_dict = {\n",
    "            name: param for name, param in param_dict.items() if param.requires_grad\n",
    "        }\n",
    "\n",
    "        decay_params = [param for param in param_dict.values() if param.dim() >= 2]\n",
    "        nodecay_params = [param for param in param_dict.values() if param.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        num_decayed_params = sum(param.numel() for param in decay_params)\n",
    "        num_nodecayed_params = sum(param.numel() for param in nodecay_params)\n",
    "        print(\n",
    "            f\"num decayed parameter tensors {len(decay_params)}, with {num_decayed_params} weights\"\n",
    "        )\n",
    "        print(\n",
    "            f\"num no decayed paramter tensors {len(nodecay_params)}, with {num_nodecayed_params} weights\"\n",
    "        )\n",
    "        fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and \"cuda\" in device\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            betas=(0.9, 0.95),\n",
    "            eps=10e-8,\n",
    "            lr=learning_rate,\n",
    "            fused=use_fused,\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T10:25:01.030479Z",
     "iopub.status.busy": "2024-08-28T10:25:01.030112Z",
     "iopub.status.idle": "2024-08-28T10:25:01.077850Z",
     "shell.execute_reply": "2024-08-28T10:25:01.076976Z",
     "shell.execute_reply.started": "2024-08-28T10:25:01.030451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        with open(\n",
    "            \"/kaggle/input/lalka/lalka-tom-pierwszy.txt\", \"r\", encoding=\"utf-8\"\n",
    "        ) as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.tokens = torch.tensor(enc.encode(text))\n",
    "        self.current_place = 0\n",
    "        print(f\"Dataset has {len(text)} characters and {len(self.tokens)} tokens\")\n",
    "        print(f\"Number of batches {len(self.tokens)//(B*T)} \")\n",
    "\n",
    "    def get_batch(self):\n",
    "        if (self.current_place + self.B * self.T + 1) >= len(self.tokens):\n",
    "            self.current_place = 0\n",
    "        x = self.tokens[\n",
    "            self.current_place : self.current_place + (self.B * self.T)\n",
    "        ].view(self.B, self.T)\n",
    "        y = self.tokens[\n",
    "            self.current_place + 1 : self.current_place + (self.B * self.T) + 1\n",
    "        ].view(self.B, self.T)\n",
    "        self.current_place += self.B * self.T\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T10:25:01.079431Z",
     "iopub.status.busy": "2024-08-28T10:25:01.079091Z",
     "iopub.status.idle": "2024-08-28T10:25:07.580458Z",
     "shell.execute_reply": "2024-08-28T10:25:07.579442Z",
     "shell.execute_reply.started": "2024-08-28T10:25:01.079403Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors 50, with 124354560 weights\n",
      "num no decayed paramter tensors 98, with 121344 weights\n",
      "using fused AdamW: True\n",
      "Dataset has 774231 characters and 428611 tokens\n",
      "Number of batches 104 \n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 524288\n",
    "B = 4\n",
    "T = 1024\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "optimizer = model.configure_optimizer(weight_decay=0.1, learning_rate=max_lr)\n",
    "\n",
    "dt = DataLoader(4, 1024)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T10:25:07.582014Z",
     "iopub.status.busy": "2024-08-28T10:25:07.581577Z",
     "iopub.status.idle": "2024-08-28T10:25:08.703818Z",
     "shell.execute_reply": "2024-08-28T10:25:08.702570Z",
     "shell.execute_reply.started": "2024-08-28T10:25:07.581988Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 28 10:25:08 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P0             26W /   70W |     653MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   39C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi\n",
    "# loss: 9.736074447631836 iter time: 1138.94 ms 3596.31 tokens/sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T10:33:31.038230Z",
     "iopub.status.busy": "2024-08-28T10:33:31.037869Z",
     "iopub.status.idle": "2024-08-28T11:00:16.048269Z",
     "shell.execute_reply": "2024-08-28T11:00:16.047245Z",
     "shell.execute_reply.started": "2024-08-28T10:33:31.038204Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 10.950180053710938 | iter time: 27919.39 ms | lr: 0.0001 | 18778.64 tokens/sec\n",
      "1 loss: 10.91910457611084 | iter time: 28782.61 ms | lr: 0.0001 | 18215.44 tokens/sec\n",
      "2 loss: 10.857242584228516 | iter time: 29410.73 ms | lr: 0.0002 | 17826.42 tokens/sec\n",
      "3 loss: 10.767234802246094 | iter time: 30048.69 ms | lr: 0.0002 | 17447.95 tokens/sec\n",
      "4 loss: 10.648353576660156 | iter time: 30396.05 ms | lr: 0.0003 | 17248.56 tokens/sec\n",
      "5 loss: 10.514158248901367 | iter time: 30571.89 ms | lr: 0.0004 | 17149.35 tokens/sec\n",
      "6 loss: 10.367039680480957 | iter time: 31191.77 ms | lr: 0.0004 | 16808.54 tokens/sec\n",
      "7 loss: 10.215703964233398 | iter time: 32311.18 ms | lr: 0.0005 | 16226.21 tokens/sec\n",
      "8 loss: 10.065608978271484 | iter time: 33480.73 ms | lr: 0.0005 | 15659.40 tokens/sec\n",
      "9 loss: 9.920387268066406 | iter time: 32672.65 ms | lr: 0.0006 | 16046.70 tokens/sec\n",
      "10 loss: 9.778135299682617 | iter time: 33105.97 ms | lr: 0.0006 | 15836.66 tokens/sec\n",
      "11 loss: 9.64661979675293 | iter time: 32670.24 ms | lr: 0.0006 | 16047.88 tokens/sec\n",
      "12 loss: 9.526154518127441 | iter time: 32616.40 ms | lr: 0.0006 | 16074.37 tokens/sec\n",
      "13 loss: 9.4059419631958 | iter time: 32717.01 ms | lr: 0.0006 | 16024.94 tokens/sec\n",
      "14 loss: 9.300597190856934 | iter time: 32592.75 ms | lr: 0.0006 | 16086.03 tokens/sec\n",
      "15 loss: 9.19599437713623 | iter time: 32580.46 ms | lr: 0.0006 | 16092.10 tokens/sec\n",
      "16 loss: 9.099540710449219 | iter time: 32587.37 ms | lr: 0.0006 | 16088.69 tokens/sec\n",
      "17 loss: 8.997427940368652 | iter time: 32547.57 ms | lr: 0.0006 | 16108.36 tokens/sec\n",
      "18 loss: 8.9166841506958 | iter time: 32544.65 ms | lr: 0.0005 | 16109.81 tokens/sec\n",
      "19 loss: 8.832512855529785 | iter time: 32596.74 ms | lr: 0.0005 | 16084.06 tokens/sec\n",
      "20 loss: 8.74542236328125 | iter time: 32596.67 ms | lr: 0.0005 | 16084.10 tokens/sec\n",
      "21 loss: 8.662225723266602 | iter time: 32482.91 ms | lr: 0.0005 | 16140.43 tokens/sec\n",
      "22 loss: 8.588750839233398 | iter time: 32412.24 ms | lr: 0.0005 | 16175.62 tokens/sec\n",
      "23 loss: 8.517206192016602 | iter time: 32409.16 ms | lr: 0.0005 | 16177.16 tokens/sec\n",
      "24 loss: 8.445281028747559 | iter time: 32255.35 ms | lr: 0.0005 | 16254.30 tokens/sec\n",
      "25 loss: 8.378650665283203 | iter time: 32204.08 ms | lr: 0.0004 | 16280.18 tokens/sec\n",
      "26 loss: 8.306424140930176 | iter time: 32350.14 ms | lr: 0.0004 | 16206.67 tokens/sec\n",
      "27 loss: 8.251080513000488 | iter time: 32114.69 ms | lr: 0.0004 | 16325.49 tokens/sec\n",
      "28 loss: 8.189094543457031 | iter time: 32001.47 ms | lr: 0.0004 | 16383.25 tokens/sec\n",
      "29 loss: 8.13507080078125 | iter time: 32077.58 ms | lr: 0.0004 | 16344.37 tokens/sec\n",
      "30 loss: 8.072721481323242 | iter time: 31935.57 ms | lr: 0.0003 | 16417.06 tokens/sec\n",
      "31 loss: 8.035562515258789 | iter time: 32025.95 ms | lr: 0.0003 | 16370.72 tokens/sec\n",
      "32 loss: 7.991600036621094 | iter time: 32194.41 ms | lr: 0.0003 | 16285.06 tokens/sec\n",
      "33 loss: 7.943583965301514 | iter time: 32291.03 ms | lr: 0.0003 | 16236.34 tokens/sec\n",
      "34 loss: 7.900812149047852 | iter time: 32497.23 ms | lr: 0.0002 | 16133.31 tokens/sec\n",
      "35 loss: 7.870370864868164 | iter time: 32624.65 ms | lr: 0.0002 | 16070.30 tokens/sec\n",
      "36 loss: 7.839836120605469 | iter time: 32517.98 ms | lr: 0.0002 | 16123.02 tokens/sec\n",
      "37 loss: 7.808119773864746 | iter time: 32466.99 ms | lr: 0.0002 | 16148.34 tokens/sec\n",
      "38 loss: 7.784275054931641 | iter time: 32459.13 ms | lr: 0.0002 | 16152.25 tokens/sec\n",
      "39 loss: 7.75223970413208 | iter time: 32543.17 ms | lr: 0.0002 | 16110.54 tokens/sec\n",
      "40 loss: 7.738762378692627 | iter time: 32550.92 ms | lr: 0.0001 | 16106.70 tokens/sec\n",
      "41 loss: 7.714807033538818 | iter time: 32498.09 ms | lr: 0.0001 | 16132.89 tokens/sec\n",
      "42 loss: 7.699973106384277 | iter time: 32538.05 ms | lr: 0.0001 | 16113.07 tokens/sec\n",
      "43 loss: 7.67392635345459 | iter time: 32493.17 ms | lr: 0.0001 | 16135.33 tokens/sec\n",
      "44 loss: 7.673026084899902 | iter time: 32450.80 ms | lr: 0.0001 | 16156.40 tokens/sec\n",
      "45 loss: 7.661453723907471 | iter time: 32565.93 ms | lr: 0.0001 | 16099.28 tokens/sec\n",
      "46 loss: 7.644004821777344 | iter time: 32581.21 ms | lr: 0.0001 | 16091.73 tokens/sec\n",
      "47 loss: 7.629480838775635 | iter time: 32480.94 ms | lr: 0.0001 | 16141.41 tokens/sec\n",
      "48 loss: 7.626075267791748 | iter time: 32500.20 ms | lr: 0.0001 | 16131.84 tokens/sec\n",
      "49 loss: 7.6188483238220215 | iter time: 32522.46 ms | lr: 0.0001 | 16120.80 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "grad_accum_steps = BATCH_SIZE // (B * T)\n",
    "for step in range(50):\n",
    "    t0 = time()\n",
    "    loss_accumulation = 0.0\n",
    "    for mini_batch in range(grad_accum_steps):\n",
    "        x, y = dt.get_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "            logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accumulation += loss.detach()\n",
    "        scaler.scale(loss).backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    lr = get_lr(step)\n",
    "    for g in optimizer.param_groups:\n",
    "        g[\"lr\"] = lr\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time()\n",
    "    t = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (dt.T * dt.B * grad_accum_steps) / (t1 - t0)\n",
    "    print(\n",
    "        f\"{step} loss: {loss_accumulation.item()} | iter time: {t:.2f} ms | lr: {lr:.4f} | {tokens_per_sec:.2f} tokens/sec\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-27T16:49:11.146223Z",
     "iopub.status.idle": "2024-08-27T16:49:11.146800Z",
     "shell.execute_reply": "2024-08-27T16:49:11.146541Z",
     "shell.execute_reply.started": "2024-08-27T16:49:11.146516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.get_device_capability(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T10:33:26.091200Z",
     "iopub.status.busy": "2024-08-28T10:33:26.090422Z",
     "iopub.status.idle": "2024-08-28T10:33:26.097210Z",
     "shell.execute_reply": "2024-08-28T10:33:26.096119Z",
     "shell.execute_reply.started": "2024-08-28T10:33:26.091167Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3235675096511841"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(1)\n",
    "a.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5372960,
     "sourceId": 8931436,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
