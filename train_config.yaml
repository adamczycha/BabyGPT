training:
  init_from: resume  # 'resume', 'gpt2', 'scratch'
  path_to_resume_training: checkpoint_0.0000.pth
  eval_only: true
  compile: true
  batch_size: 524288  # in tokens
  mini_batch: 40  # real batch size used to simulate bigger batches

optimizer:
  warmup_steps: 10
  max_steps: 50
  max_lr: 6e-4
  min_lr: 0.1  # min_lr is used as fraction min_lr = (min_lr * max_lr)

model:
  block_size: 1024
  vocab_size: 50257
  n_head: 12
  n_layer: 12
  n_embd: 768

saving:
  save_checkpoints: true
  save_every_n_batches: 10
  save_end_model: true
  save_with_resume_option: true

data:
  dataset: fineweb
  # chank_size is set to ddp_world_size * block_size * batchsize to work nicely with DDP (you can overwrite)
  # chank_size: your value