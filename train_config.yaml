training:
  init_from: scratch  # 'resume', 'gpt2', 'scratch'
  path_to_resume_training: checkpoint_0.0000.pth
  training: true
  sample: true
  eval: false
  compile: true
  batch_size: 327680  # in tokens
  mini_batch: 10  # real batch size used to simulate bigger batches

evaluation:
  validation_every_n_steps: 250
  validation_micro_steps: 5
  sample_every_n: 50
  hellaswag_every_n_steps: 250

optimizer:
  warmup_steps: 10
  max_steps: 50
  max_lr: 6e-4
  min_lr: 0.1  # min_lr is used as fraction min_lr = (min_lr * max_lr)

model:
  block_size: 1024
  vocab_size: 50257
  n_head: 12
  n_layer: 12
  n_embd: 768
  kv_group_factor: 2 # every n queries has one k, v projection


saving:
  save_checkpoints: false
  save_every_n_batches: 250
  save_end_model: true
  save_with_resume_option: true

data:
  dataset: wojny_husyckie
  # chank_size is set to ddp_world_size * block_size * batchsize to work nicely with DDP (you can overwrite)
  # chank_size: your value