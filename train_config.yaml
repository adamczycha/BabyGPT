training:
  init_from: 'scratch'  # 'resume', 'gpt2', 'scratch'
  path_to_resume_training: checkpoint_0.8750.pth
  training: true
  sample: false
  eval: true
  compile: true
  batch_size: 573440  # in tokens
  mini_batch:  40   # real batch size used to simulate bigger batches

evaluation:
  validation_every_n_steps: 200
  validation_micro_steps: 1
  sample_every_n: 50
  force_sample: true
  samples_per_rank: 1
  length: 32 
  hellaswag_every_n_steps: 1000

optimizer:
  warmup_steps: 2000
  max_steps: 20000 # 28B tokens 20k*0.5M => 0.37 epoch
  max_lr: 6e-4
  min_lr: 0.1  # min_lr is used as fraction min_lr = (min_lr * max_lr)

model:
  block_size: 1024
  vocab_size: 50257
  n_head: 12
  n_layer: 12
  n_embd: 768
  kv_group_factor: 2 # every n queries has one k, v projection


saving:
  save_checkpoints: false
  save_every_n_batches: 250
  save_end_model: true
  save_with_resume_option: true

data:
  dataset: wojny_husyckie
  hellaswag_file: hellaswag_val_PL.jsonl 
  # chank_size is set to ddp_world_size * block_size * batchsize to work nicely with DDP (you can overwrite)
  # chank_size: your value