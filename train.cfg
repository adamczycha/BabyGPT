[training]
# 'resume', 'gpt2'
init_from = resume
path_to_resume_training= checkpoint_0.0000.pth
eval_only = True
compile = True
# in tokens
batch_size = 524288   
#real batch size used to simulate bigger batches
mini_batch = 35  

[optimizer]
warmup_steps = 10
max_steps = 50
max_lr = 6e-4
# min_lr is used as fraction min_lr = (min_lr * max_lr) 
min_lr = 0.1

[model]
block_size = 1024
vocab_size = 50257
n_head = 12
n_layer = 12
n_embd  = 768

[saving]
save_checkpoints = True
save_every_n_batches = 10
save_end_model = True
save_with_resume_option = True 

[data]
dataset = fineweb
# chank_size is set to ddp_world_size * block_size * batchsize to work nicly with DDP (you can overwrite)
# chank_size = your value