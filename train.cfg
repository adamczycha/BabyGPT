[training]
# 'resume', 'gpt2'
init_from = scratch
eval_only = True
compile = True
# in tokens
batch_size = 524288   
#real batch size used to simulate bigger batches
mini_batch = 24  

[optimizer]
warmup_steps = 10
max_steps = 50
max_lr = 6e-4
min_lr = max_lr * 0.1

[model]
block_size = 1024
vocab_size = 50257
n_head = 12
n_layer = 12
n_embd  = 768

[saving]
save_checkoints = True
save_every_n_batches = 1000
save_end_model = True
save_with_optimizer = True 

[data]
dataset = fineweb