{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T17:40:14.191351Z","iopub.status.busy":"2024-08-13T17:40:14.190853Z","iopub.status.idle":"2024-08-13T17:40:14.291230Z","shell.execute_reply":"2024-08-13T17:40:14.289722Z","shell.execute_reply.started":"2024-08-13T17:40:14.191313Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/input/lalka/lalka-tom-pierwszy.txt', 'r',encoding='utf-8') as f:\n","    lalka1 =  f.read()\n","\n","with open('/kaggle/input/lalka/lalka-tom-drugi.txt', 'r',encoding='utf-8') as f:\n","    lalka2 =  f.read()\n","\n","text = lalka1 + lalka2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from kar"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T18:03:02.590373Z","iopub.status.busy":"2024-08-12T18:03:02.589907Z","iopub.status.idle":"2024-08-12T18:03:02.605640Z","shell.execute_reply":"2024-08-12T18:03:02.603898Z","shell.execute_reply.started":"2024-08-12T18:03:02.590324Z"},"trusted":true},"outputs":[],"source":["def merge(tokens, pair, idx):\n","    new_tokens = []\n","    i = 0\n","    while i < len(tokens):\n","        if i < len(tokens)-1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n","            new_tokens.append(idx)\n","            i +=2\n","        else:\n","            new_tokens.append(tokens[i])\n","            i+=1\n","    return new_tokens\n","\n","def get_stats(tokens, update_stats = None):\n","    stats = {} if update_stats is None else update_stats\n","    if len(tokens) >= 2:\n","        for pair in zip(tokens, tokens[1:]):\n","            stats[pair] = stats.get(pair, 0) + 1\n","    return stats\n","\n","        \n","def render_token(t: bytes) -> str:\n","    s = t.decode('utf-8', errors='replace')\n","    return s"]},{"cell_type":"code","execution_count":194,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T19:36:16.270455Z","iopub.status.busy":"2024-08-12T19:36:16.269988Z","iopub.status.idle":"2024-08-12T19:36:16.291442Z","shell.execute_reply":"2024-08-12T19:36:16.290015Z","shell.execute_reply.started":"2024-08-12T19:36:16.270422Z"},"trusted":true},"outputs":[],"source":["class Tokenizer:\n","    def __init__(self):\n","        self.merges = {}\n","        self.pattern = \"\"\n","        self.special_tokens = {}\n","        self.vocab = self._build_vocab()\n","        \n","    def _version(self):\n","        return 'base_tokenizer_v1'\n","        \n","    def train(self, text, vocab_size, verbose = False):\n","        raise NotImplementedError\n","    \n","    def encode(self, text):\n","        raise NotImplementedError\n","        \n","    def decode(self, text):\n","        raise NotImplementedError\n","        \n","    def _build_vocab(self):\n","        vocab = {idx: bytes([idx]) for idx in range(256)}\n","        for (p0,p1), idx in self.merges.items():\n","            vocab[idx] = vocab[p0] + vocab[p1]\n","        for special, idx in self.special_tokens.items():\n","            vocab[idx] = special.encode('utf8')\n","        return vocab\n","    \n","    def save(self, path):\n","        with open(path+'.model', 'w') as f:\n","            f.write(f'{self._version()}\\n')\n","            f.write(f'{self.pattern}\\n')\n","            f.write(f'{len(self.special_tokens)}\\n')\n","            for special, idx in self.special_tokens.items():\n","                f.write(f'{special} {idx}\\n')\n","            for idx1, idx2 in self.merges:\n","                f.write(f'{idx1} {idx2}\\n')\n","        inverted_merges = {v:pair for pair,v in self.merges.items()}\n","        with open(path + '.vocab', 'w', encoding='utf-8') as f:\n","            for idx, token in self.vocab.items():\n","                s = render_token(token)\n","                if idx in inverted_merges:\n","                    idx0, idx1 = inverted_merges[idx]\n","                    s0 = render_token(self.vocab[idx0])\n","                    s1 = render_token(self.vocab[idx1])\n","                    f.write(f'[{s0}][{s1}] -> [{s}] {idx}\\n')\n","                else:\n","                    f.write(f\"[{s}] {idx}\\n\")\n","    \n","    def load(self, path):\n","        idx = 256\n","        merges ={}\n","        special_tokens = {}\n","        with open(path+'.model', 'r') as f:\n","            version_control = f.readline().strip()\n","            assert version_control == f'{self._version}', 'Wrong file, tokenizer version does not much'\n","            self.pattern = f.readline().strip()\n","            special_tokens_length = int(f.readline().strip())\n","            for _ in range(special_tokens_length):\n","                special, special_idx = f.readline().strip().split()\n","                special_tokens[special] = int(special_idx)\n","            for line in f:\n","                idx1, idx2 = map(int, line.split())\n","                merges[(idx1, idx2)] = idx\n","                idx += 1\n","        self.merges = merges\n","        self.special_tokens = special_tokens\n","        self.vocab = self._build_vocab()\n","            \n","            \n","            \n","        "]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T18:03:02.646043Z","iopub.status.busy":"2024-08-12T18:03:02.645568Z","iopub.status.idle":"2024-08-12T18:03:02.663997Z","shell.execute_reply":"2024-08-12T18:03:02.661829Z","shell.execute_reply.started":"2024-08-12T18:03:02.646001Z"},"trusted":true},"outputs":[],"source":["class BasicTokenizer(Tokenizer):\n","    def __init__(self):\n","        super().__init__()\n","    \n","    def train(self, text, vocab_size, verbose = False):\n","        tokens = text.encode('utf-8')\n","        tokens = list(tokens)\n","        \n","        idx = 256\n","        merges = {}\n","        vocab = {idx: bytes([idx]) for idx in range(256)}\n","        while idx < vocab_size:\n","            stats = get_stats(tokens)\n","            candidate = max(stats, key= stats.get)\n","            merges[candidate] = idx\n","            vocab[idx] = vocab[candidate[0]] + vocab[candidate[1]]\n","            tokens = merge(tokens, candidate, idx)\n","            \n","            if verbose: \n","                print(f\"{candidate[0]} + {candidate[1]} => {idx}\")\n","            idx +=1\n","        self.merges = merges\n","        self.vocab = vocab\n","\n","    def encode(self, text):\n","        tokens = text.encode('utf-8')\n","        tokens = list(tokens)\n","        \n","        \n","        no_more = False\n","        while no_more:\n","            stats = get_stats(tokens)\n","            candidate = min(stats, key= lambda x: self.merges.get(x, float('inf')))\n","            \n","            if candidate not in self.merges:\n","                no_more = True\n","            \n","            tokens = merge(tokens, candidate, self.merges[candidate])\n","        return tokens\n","            \n","        \n","    def decode(self, tokens):\n","        text_bytes = b\"\".join(self.vocab[idx] for idx in tokens)\n","        text = text_bytes.decode('utf-8', errors = 'replace')\n","        return text"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T18:03:02.678048Z","iopub.status.busy":"2024-08-12T18:03:02.677571Z","iopub.status.idle":"2024-08-12T18:03:02.685902Z","shell.execute_reply":"2024-08-12T18:03:02.684010Z","shell.execute_reply.started":"2024-08-12T18:03:02.678014Z"},"trusted":true},"outputs":[],"source":["import regex as re"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T18:03:02.704476Z","iopub.status.busy":"2024-08-12T18:03:02.703971Z","iopub.status.idle":"2024-08-12T18:03:02.732960Z","shell.execute_reply":"2024-08-12T18:03:02.731164Z","shell.execute_reply.started":"2024-08-12T18:03:02.704441Z"},"trusted":true},"outputs":[],"source":["GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n","GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","\n","class RegexTokenizer(BasicTokenizer):\n","    def __init__(self, pattern = None):\n","        super().__init__()\n","        self.regex_pattern =  GPT4_SPLIT_PATTERN if pattern is None else pattern\n","        self.compiled_pattern = re.compile(self.regex_pattern)\n","        self.special_tokens = {}\n","        self.invers_special_tokens = {}\n","        \n","    \n","    def train(self, text, vocab_size, verbose = False):\n","        \n","        chunked_text = re.findall(self.compiled_pattern, text)\n","        chunked_tokens = [list(chunk.encode('utf-8')) for chunk in chunked_text ]\n","\n","        idx = 256\n","        merges = {}\n","        vocab = {idx: bytes([idx]) for idx in range(256)}\n","        while idx < vocab_size:\n","            #update count of connected tokens for every token group\n","            stats = {}\n","            for tokens in chunked_tokens:\n","                get_stats(tokens,stats)\n","            \n","            candidate = max(stats, key= stats.get)\n","            \n","            chunked_tokens = [merge(tokens, candidate, idx) for tokens in chunked_tokens]\n","                    \n","            merges[candidate] = idx\n","            vocab[idx] = vocab[candidate[0]] + vocab[candidate[1]]\n","            \n","            if verbose: \n","                print(f\"{candidate[0]} + {candidate[1]} => {idx}\")\n","            idx +=1\n","        self.merges = merges\n","        self.vocab = vocab\n","        \n","    # special tokens in the form of vocab dictionary {<ENDOFTEXT>:50000,....}\n","    def special_token_registry(self, special = None):\n","        if special is not None:\n","            self.special_tokens = special \n","            self.invers_special_tokens = {v:k for k,v in special.items()}\n","            self.vocab.update(self.invers_special_tokens)\n","        \n","    # encode pure utf-8 chunk of bytes\n","    def _encode_chunk(self, text_bytes):\n","        \n","        tokens = list(text_bytes)\n","        while len(tokens) >= 2:\n","            stats = get_stats(tokens)\n","            candidate = min(stats, key= lambda x: self.merges.get(x, float('inf')))\n","            \n","            if candidate not in self.merges:\n","                break\n","                            \n","            tokens = merge(tokens, candidate, self.merges[candidate])\n","        return tokens\n","    \n","    # encode text without special tokens\n","    def _encode_ordinary(self, text):\n","        chunked_test = re.findall(self.compiled_pattern, text)\n","        \n","        tokens = []\n","        for chunk in chunked_test:\n","            chunk = chunk.encode('utf-8')\n","            tokens.extend(self._encode_chunk(chunk))\n","        \n","        return tokens\n","                    \n","                    \n","    def encode(self, text, allowed_special = 'none_raise'):\n","                        \n","        if allowed_special == 'all':\n","            special = self.special_tokens\n","        elif allowed_special == 'none':\n","            special = {}\n","        elif allowed_special == 'none_raise':\n","            special = {}\n","            assert all(token not in text for token in self.special_tokens) \n","        else:\n","            raise ValueError(f\"allowed_special={allowed_special} not understood\")\n","                            \n","        if not special:\n","            return self._encode_ordinary(text)\n","        \n","        special_pattern = '(' + \"|\".join(re.escape(k) for k in special) + \")\"\n","        special_chunked = re.split(special_pattern, text)\n","        \n","        tokens = []\n","        for chunk in special_chunked:\n","            if chunk in special:\n","                tokens.extend([special[chunk]])\n","            else:\n","                tokens.extend(self._encode_ordinary(chunk))\n","        return tokens\n","    \n","    def decode(self, tokens):\n","        text_bytes = b\"\"\n","        for idx in tokens:\n","            if idx in self.invers_special_tokens:\n","                text_bytes += self.vocab[idx].encode('utf-8')\n","            else:\n","                text_bytes += self.vocab[idx]\n","        text = text_bytes.decode('utf-8', errors = 'replace')\n","        return text"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T18:03:02.736480Z","iopub.status.busy":"2024-08-12T18:03:02.735922Z","iopub.status.idle":"2024-08-12T18:03:18.283762Z","shell.execute_reply":"2024-08-12T18:03:18.281954Z","shell.execute_reply.started":"2024-08-12T18:03:02.736433Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.7.0)\n","Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\n","Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n"]}],"source":["!pip install tiktoken"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T18:03:18.286690Z","iopub.status.busy":"2024-08-12T18:03:18.286140Z","iopub.status.idle":"2024-08-12T18:03:18.294010Z","shell.execute_reply":"2024-08-12T18:03:18.292508Z","shell.execute_reply.started":"2024-08-12T18:03:18.286647Z"},"trusted":true},"outputs":[],"source":["import tiktoken\n"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T18:03:18.298310Z","iopub.status.busy":"2024-08-12T18:03:18.297650Z","iopub.status.idle":"2024-08-12T18:03:18.317120Z","shell.execute_reply":"2024-08-12T18:03:18.315353Z","shell.execute_reply.started":"2024-08-12T18:03:18.298235Z"},"trusted":true},"outputs":[],"source":["def bpe(mergeable_ranks, token, max_rank = None):\n","    parts = [bytes([b]) for b in token]\n","    while True:\n","        min_rank = None\n","        min_idx = None\n","        for i , pair in enumerate(zip(parts[:-1], parts[1:])):\n","            rank = mergeable_ranks.get(pair[0] + pair[1])\n","            if rank is not None and (min_rank is None or rank < min_rank):\n","                min_rank = rank\n","                min_idx = i\n","        if min_rank is None or (min_rank is not None and min_rank >= max_rank) :\n","            break\n","        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx+1]] + parts[min_idx + 2:]\n","    return parts\n","\n","def recover_merges(mergeable_ranks):\n","    merges = {}\n","    for byte, rank in mergeable_ranks.items():\n","        if len(byte) < 2:\n","            continue\n","        pair = tuple(bpe(mergeable_ranks, byte, rank))\n","        \n","        idx0 = mergeable_ranks[pair[0]]\n","        idx1 = mergeable_ranks[pair[1]]\n","        merges[(idx0, idx1)] = rank\n","        \n","    return merges"]},{"cell_type":"code","execution_count":197,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T19:41:53.164514Z","iopub.status.busy":"2024-08-12T19:41:53.164107Z","iopub.status.idle":"2024-08-12T19:41:53.179968Z","shell.execute_reply":"2024-08-12T19:41:53.178859Z","shell.execute_reply.started":"2024-08-12T19:41:53.164483Z"},"trusted":true},"outputs":[],"source":["GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","GPT4_SPECIAL_TOKENS = {\n","    '<|endoftext|>': 100257,\n","    '<|fim_prefix|>': 100258,\n","    '<|fim_middle|>': 100259,\n","    '<|fim_suffix|>': 100260,\n","    '<|endofprompt|>': 100276\n","}\n","\n","class GPT4Tokenizer(RegexTokenizer):\n","    def __init__(self):\n","        super().__init__(pattern =GPT4_SPLIT_PATTERN)\n","        enc = tiktoken.get_encoding(\"cl100k_base\")\n","        self.mergeable_ranks = enc._mergeable_ranks\n","        self.merges = recover_merges(self.mergeable_ranks)\n","        self.byte_shuffle = {self.mergeable_ranks[bytes([i])]:i  for i in range(256)}\n","        self.byte_reshuffle = {v:k for k,v in self.byte_shuffle.items()}\n","        \n","        #change order of first 256 characters and update all other merges to be compatilbe with new order.\n","        self.merges = self.shuffle_merges()\n","        \n","        \n","        self.special_tokens = GPT4_SPECIAL_TOKENS\n","        self.vocab = self._build_vocab()\n","    \n","    def shuffle_merges(self):\n","        merges_keys = []\n","        for (p0,p1) in list(self.merges.keys()):\n","            if p0 <=255:\n","                 p0 = self.byte_shuffle[p0]\n","            if p1 <=255:\n","                p1 = self.byte_shuffle[p1]\n","            merges_keys.append((p0,p1))\n","        shuffled_merges = {merges_keys[i]:i+256 for i in range(len(self.merges))}\n","        return shuffled_merges\n","    \n","    def train(self, text, vocab_size, verbose=False):\n","        raise NotImplementedError\n","        \n","    def encode(self, text, allowed_special = 'none_raise'):\n","        shuffled_tokens = super().encode(text,allowed_special)\n","        return [self.byte_reshuffle[token] if token <= 255 else token for token in shuffled_tokens]\n","    \n","    def decode(self, tokens):\n","        shuffled_tokens = [self.byte_shuffle[token] if token <= 255 else token for token in tokens]\n","        return super().decode(shuffled_tokens)\n","    \n","    def save(self, path):\n","        raise NotImplementedError(\"GPT4Tokenizer is loaded from tiktoken so saving is useless.\")\n","        \n","    def load(self, path):\n","        raise NotImplementedError(\"GPT4Tokenizer is loaded from tiktoken at initialization \")"]},{"cell_type":"code","execution_count":198,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T19:41:54.092634Z","iopub.status.busy":"2024-08-12T19:41:54.092230Z","iopub.status.idle":"2024-08-12T19:41:57.471250Z","shell.execute_reply":"2024-08-12T19:41:57.470306Z","shell.execute_reply.started":"2024-08-12T19:41:54.092603Z"},"trusted":true},"outputs":[],"source":["gpt4tok = GPT4Tokenizer()"]},{"cell_type":"code","execution_count":187,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T19:23:32.153956Z","iopub.status.busy":"2024-08-12T19:23:32.153613Z","iopub.status.idle":"2024-08-12T19:23:32.161904Z","shell.execute_reply":"2024-08-12T19:23:32.160776Z","shell.execute_reply.started":"2024-08-12T19:23:32.153927Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'You can define a dictionary using curly brackets.'"]},"execution_count":187,"metadata":{},"output_type":"execute_result"}],"source":["gpt4tok.decode(gpt4tok.encode('You can define a dictionary using curly brackets.'))"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T17:38:35.091623Z","iopub.status.busy":"2024-08-13T17:38:35.091125Z","iopub.status.idle":"2024-08-13T17:38:35.152726Z","shell.execute_reply":"2024-08-13T17:38:35.151216Z","shell.execute_reply.started":"2024-08-13T17:38:35.091585Z"},"trusted":true},"outputs":[],"source":["import sentencepiece"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T17:39:28.886755Z","iopub.status.busy":"2024-08-13T17:39:28.886279Z","iopub.status.idle":"2024-08-13T17:39:28.893154Z","shell.execute_reply":"2024-08-13T17:39:28.891485Z","shell.execute_reply.started":"2024-08-13T17:39:28.886720Z"},"trusted":true},"outputs":[],"source":["import sentencepiece as spm\n","s = spm.SentencePieceProcessor()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T18:21:07.573684Z","iopub.status.busy":"2024-08-13T18:21:07.573175Z","iopub.status.idle":"2024-08-13T18:21:07.625091Z","shell.execute_reply":"2024-08-13T18:21:07.623668Z","shell.execute_reply.started":"2024-08-13T18:21:07.573648Z"},"trusted":true},"outputs":[{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["sp = spm.SentencePieceProcessor()\n","sp.load(\"/kaggle/input/tokenizer/tokenizer.model\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T18:27:40.415595Z","iopub.status.busy":"2024-08-13T18:27:40.415108Z","iopub.status.idle":"2024-08-13T18:27:40.589784Z","shell.execute_reply":"2024-08-13T18:27:40.587807Z","shell.execute_reply.started":"2024-08-13T18:27:40.415556Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocab size: 32000\n","Character coverage: 0.9999499917030334\n","Model type: 2\n","Input sentence size: 200000000\n","Max sentence length: 4192\n","Mining sentence size: 0\n"]},{"ename":"AttributeError","evalue":"training_algorithm","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax sentence length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_proto\u001b[38;5;241m.\u001b[39mtrainer_spec\u001b[38;5;241m.\u001b[39mmax_sentence_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMining sentence size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_proto\u001b[38;5;241m.\u001b[39mtrainer_spec\u001b[38;5;241m.\u001b[39mmining_sentence_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining algorithm: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel_proto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_algorithm\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormalization rule name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_proto\u001b[38;5;241m.\u001b[39mnormalizer_spec\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: training_algorithm"]}],"source":["import sentencepiece as spm\n","from sentencepiece import sentencepiece_model_pb2 as model_pb2\n","\n","# Load the SentencePiece model\n","sp = spm.SentencePieceProcessor()\n","sp.Load(\"/kaggle/input/tokenizer/tokenizer.model\")\n","\n","# Get the model proto\n","model_proto = model_pb2.ModelProto()\n","model_proto.ParseFromString(sp.serialized_model_proto())\n","\n","# Now you can access various training parameters\n","print(f\"Vocab size: {model_proto.trainer_spec.vocab_size}\")\n","print(f\"Character coverage: {model_proto.trainer_spec.character_coverage}\")\n","print(f\"Model type: {model_proto.trainer_spec.model_type}\")\n","print(f\"Input sentence size: {model_proto.trainer_spec.input_sentence_size}\")\n","print(f\"Max sentence length: {model_proto.trainer_spec.max_sentence_length}\")\n","print(f\"Mining sentence size: {model_proto.trainer_spec.mining_sentence_size}\")\n","print(f\"Training algorithm: {model_proto.trainer_spec.training_algorithm}\")\n","print(f\"Normalization rule name: {model_proto.normalizer_spec.name}\")\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-13T18:29:04.917027Z","iopub.status.busy":"2024-08-13T18:29:04.916551Z","iopub.status.idle":"2024-08-13T18:29:04.924344Z","shell.execute_reply":"2024-08-13T18:29:04.922752Z","shell.execute_reply.started":"2024-08-13T18:29:04.916989Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["input: \"/large_experiments/theorem/datasets/MERGED/all.test1.merged\"\n","model_prefix: \"spm_model_32k_200M_charcov099995_allowWSO__v2\"\n","model_type: BPE\n","vocab_size: 32000\n","self_test_sample_size: 0\n","input_format: \"text\"\n","character_coverage: 0.9999499917030334\n","input_sentence_size: 200000000\n","seed_sentencepiece_size: 1000000\n","shrinking_factor: 0.75\n","num_threads: 80\n","num_sub_iterations: 2\n","max_sentence_length: 4192\n","shuffle_input_sentence: true\n","max_sentencepiece_length: 16\n","split_by_unicode_script: true\n","split_by_whitespace: true\n","split_by_number: true\n","treat_whitespace_as_suffix: false\n","split_digits: true\n","allow_whitespace_only_pieces: true\n","vocabulary_output_piece_score: true\n","hard_vocab_limit: true\n","use_all_vocab: false\n","byte_fallback: true\n","required_chars: \"\"\n","unk_id: 0\n","bos_id: 1\n","eos_id: 2\n","pad_id: -1\n","unk_surface: \" \\342\\201\\207 \"\n","unk_piece: \"<unk>\"\n","bos_piece: \"<s>\"\n","eos_piece: \"</s>\"\n","pad_piece: \"<pad>\"\n","train_extremely_large_corpus: false\n","enable_differential_privacy: false\n","differential_privacy_noise_level: 0.0\n","differential_privacy_clipping_threshold: 0\n","\n"]}],"source":["print( model_proto.trainer_spec)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5372960,"sourceId":8931436,"sourceType":"datasetVersion"},{"datasetId":5539711,"sourceId":9167843,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":4}
